\documentclass[twoside,11pt]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{subcaption}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween}

\pgfplotsset{
  compat=1.3,
  every non boxed x axis/.style={
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\def\perc{\texttt{perco\-late}}
\def\v{\texttt{v0.1.0}}

\def\titl{Programming Skills coursework II b: benchmarking
  \perc{} \v{} with different densities}

\title{\titl}

\author{}

\ShortHeadings{B160509}{B160509}
\firstpageno{1}


\begin{document}

\maketitle

\begin{abstract}
  \perc{} \v{} is a scientific program.
  It generates a random matrix with two kinds of cells:
  empty and filled.
  Empty cells build clusters with their neighbors and
  \perc{} finds clusters that begin at the first column and
  end at the last.
  If such a cluster exists, the matrix percolates.

  The benchmark of \perc{} looks at how the program
  scales with different densities of filled cells.
  The goal of this benchmark is to find bottlenecks of
  the execution time, in order to optimize \perc{} in
  coming releases.

  The benchmark was executed on the Cirrus supercomputer
  with exclusive access to one back end node.
  The results of the benchmark show extreme scaling
  behavior of \perc{}, but successfully exposes the
  bottleneck.

  This paper discusses the results of the benchmark and
  gives an outline for optimizing \perc{} in future
  releases.
\end{abstract}

\begin{keywords}
Scientific programming, performance optimization
\end{keywords}

\section{Introduction} % {{{

This paper documents the results of a benchmark performed
on \perc{} \v{}.
\perc{} is a scientific program written in the
Fortran programming language. It generates a random matrix
with two kinds of cells: empty and filled.
Empty cells build clusters with their neighbors and
\perc{} finds clusters that begin at the first column and
end at the last.
If such a cluster exists, the matrix percolates.

One way to configure \perc{} is by setting the density of
the empty cells.
This benchmark looks at the behavior of \perc{} with
different densities of empty cells and how the density
influences the execution time.
The goal is to find the bottlenecks concerning the
execution time, in order to optimize \perc{} and make it
perform better, executing faster.

The benchmark was performed on the Cirrus supercomputer
with exclusive access to one back end node
\citep[see][]{cirrus}.
The program was compiled with the GNU Fortran Compiler
(\texttt{gfortran}) version 4.8.5, with the maximum
optimization provided (optimization level \texttt{O3})
\citep[see][]{gfortran}.

This paper begins by describing \perc{} \v{} and the
conducted benchmark.
Afterwards the results are presented and discussed.
At last a conclusion is drawn and next steps for a
follow-up benchmark are outlined.
% }}}

\section{Method} % {{{

Let $n \in \mathbb{N}$ be a positive integer.
Let $A: n \times n$ be a matrix,
$A \in \mathbb{N}_{0}^{n \times n}$.
$A(i, j); 1 \leq i, j \leq n; i, j \in \mathbb{N}$ is
the cell of $A$ in the $i$th row and the $j$th column.
Let $S := \{filled, empty\}$ be a binary set containing the
two states a cell of $A$ can have and let $\sigma:
\mathbb{N}_0 \rightarrow S$ be a function that
maps a cell of $A$ to its state:
\begin{align*}
\sigma(x) = \begin{cases}
  filled &\text{if } x = 0 \\
  empty  &\text{otherwise}
\end{cases}.
\end{align*}
Let $A': n+2 \times n+2$ be a version of $A$ with a halo
containing filled cells:
\begin{align}
  \label{eq:a'}
i,j = 0, \dots, n+1: A'(i, j) := \begin{cases}
  0 &\text{if } i=0 \lor j=0 \lor i=n+1 \lor j=n+1 \\
  A(i, j) &\text{otherwise}
\end{cases}.
\end{align}
The density of the empty cells $\rho$ of $A$ is determined
by the following function:
\begin{align*}
\rho(A) := \frac{|\{i,j=1,\dots,n: \sigma(A(i,j)) = empty\}|}{n^2}.
\end{align*}

\perc{} \v{} randomly initializes $A$ with empty and
filled cells.
The empty cells are taken continuously from the sequence
$1,2,\dots,n^2$.
So, after initialization the empty cells of $A$ are
$1,\dots,|\{i,j=1,\dots,n:\sigma(A(i,j))=empty\}|$, which
are randomly distributed over $A$.
Afterwards the program sets $A'$ according to (\ref{eq:a'})
(see Algorithm~\ref{alg:perc}, lines 1--2).
After initialization, clusters in $A'$ are build
iteratively (see Algorithm~\ref{alg:build_clusters}).
This is done by setting all empty cells of $A'$ to their
biggest neighbor.
Let $\mu:\mathbb{N}_0^{m \times m} \times \mathbb{N} \times
\mathbb{N} \rightarrow \mathbb{N}_0; m \in \mathbb{N}$ be
the function returning the biggest value of all neighboring
cells and the current cell:
\begin{align*}
  \mu(A, i, j) := \max(A(i, j), A(i-1, j), A(i+1, j),
                       A(i, j-1), A(i, j+1)).
\end{align*}
The clustering is finished, once no cell changes value
anymore (see Algorithm~\ref{alg:build_clusters}, line 2).

After the clustering the clusters are sorted in descending
order based on their size
($size: \mathbb{N}_0^{n \times n} \times \mathbb{N}
\rightarrow \mathbb{N}_0; size(A, x) :=
|\{i,j=1,\dots,n: A(i,j) = x\}|$)(see
Algorithm~\ref{alg:perc}, line 5).
Sorting is done using the quicksort algorithm
\citep[see][]{hoare_1961}.

Afterwards every empty cell of $A$ is mapped to its index
in the array containing the sorted clusters.
Every filled cell is mapped to $|clusters| + 1$.
The result of this operation is the color map (see
Algorithm~\ref{alg:perc}, line 6).
The color map is needed for writing a Portable Gray Map
Image (PGM).

After doing this operations, \perc{} writes a log to
\texttt{stdout} and writes $A$ and the color map to files.
These io-operations are not measured during the benchmark.

\begin{algorithm}
  \caption{: \perc{}}
  \label{alg:perc}

  \begin{algorithmic}[1]
    \STATE{randomly initialize $A$}
    \STATE{set $A'$ according to (\ref{eq:a'})}
    \STATE{build\_clusters($A'$)}
    \STATE{$A := A'(1:n, 1:n)$}
    \STATE{sort the clusters based on their size}
    \STATE{build the color map}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{: build\_clusters($A'$)}
  \label{alg:build_clusters}

  \begin{algorithmic}[1]
    \STATE{$A''(i, j) := 0; i,j = 0,\dots,n+1$}
    \WHILE{$\Sigma_{i,j=1}^{n}(A'(i,j) - A''(i, j)) > 0$}
      \STATE{$A'' := A'$}
      \FOR{$i, j=1,\dots,n$}
        \STATE{$A'(i, j) = \begin{cases}
          \mu(A', i, j) &\text{if } \sigma(A'(i,j)) = empty
          \\
          0 &\text{otherwise}
        \end{cases}$}
      \ENDFOR
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

For the benchmark, $n$ was set to $2000$.
\perc{} offers an interface which controls the
initialization behavior.
One can define a density $\rho_{goal}$ and a seed which is
passed to the pseudo random number generator.
During initialization, $\rho_{goal}$ is approximated.

The benchmark ran \perc{} with $\rho_{goal} :=
0.01,0.02,\dots,0.99$. Every $\rho_{goal}$ was tried with
five seeds, resulting in 495 distinct measures, with an
approximately uniform distribution over $\rho$ generated
during the initialization.

The three operations: clustering, sorting and building the
color map were measured (see Algorithm~\ref{alg:perc},
lines 3ff).
The sum of all three measures is the execution time
(io-operations and initialization excluded).

The execution time is measured in seconds.
OpenMP 4.5's \texttt{omp\_get\_wtime} function was used
for the time measuring \citep[see][Chapter 3.4.1]{openmp}.

% }}}

\section{Results} % {{{

% 1st part fig {{{
\begin{figure}[htbp]
\begin{center}
\noindent
% density -> time_sum {{{
\begin{subfigure}[t]{0.49\textwidth}
\begin{tikzpicture}[scale=1]
  \datavisualization[
    scientific axes=clean,
    visualize as scatter/.list={d},
    d={style={mark=x, mark size=1.5pt}},
    y axis={include value={0}, label={time in seconds $t$}},
    x axis={include value={0,1}, label={$\rho$}},
  ]
  data[headline={x, y}, read from file=data/density_time_sum.csv, set=d]
  ;
\end{tikzpicture}
\caption{Scatter plot mapping $\rho$ to the execution time
  of Algorithm~\ref{alg:perc}.}
\label{fig:p1:a}
\end{subfigure}
% }}}
% kde time_sum {{{
\begin{subfigure}[t]{0.49\textwidth}
\begin{tikzpicture}[scale=1]
  \datavisualization[
    scientific axes=clean,
    visualize as line/.list={d},
    y axis={include value={0}, label={P(Time=$t$)}},
    x axis={include value={0}, label={time in seconds $t$}},
  ]
  data[headline={x, y}, read from file=data/kde_time_sum.csv, set=d]
  ;
\end{tikzpicture}
\caption{Gaussian kernel density estimation of the
  execution time. Clearly shows that
  Algorithm~\ref{alg:perc} favors extreme
  times (the function has its maxima at the edges). }
\label{fig:p1:b}
\end{subfigure}
% }}}

% density -> num_iter {{{
\begin{subfigure}[t]{0.49\textwidth}
\begin{tikzpicture}[scale=1]
  \datavisualization[
    scientific axes=clean,
    visualize as scatter/.list={d},
    d={style={mark=x, mark size=1.5pt}},
    y axis={include value={0}, label={amount of iterations $\#i$}},
    x axis={include value={0,1}, label={$\rho$}},
  ]
  data[headline={x, y}, read from file=data/density_num_iter.csv, set=d]
  ;
\end{tikzpicture}
\caption{Scatter plot mapping $\rho$ to the amount of
  iterations of Algorithm~\ref{alg:build_clusters}.}
\label{fig:p1:c}
\end{subfigure}
% }}}
% kde num_iter {{{
\begin{subfigure}[t]{0.49\textwidth}
\begin{tikzpicture}[scale=1]
  \datavisualization[
    scientific axes=clean,
    visualize as line/.list={d},
    y axis={include value={0}, label={P(Iter=$\#i$)}},
    x axis={include value={0}, label={amount of iterations $\#i$}},
  ]
  data[headline={x, y}, read from file=data/kde_num_iter.csv, set=d]
  ;
\end{tikzpicture}
\caption{Gaussian kernel density estimation of the
  amount of iterations. Clearly shows that
  Algorithm~\ref{alg:build_clusters} favors extreme amounts
  of iterations (the function has its maxima at the edges).
}
\label{fig:p1:d}
\end{subfigure}
% }}}

% time_sum -> num_iter {{{
\begin{subfigure}[b]{0.49\textwidth}
\begin{tikzpicture}[scale=1]
  \datavisualization[
    scientific axes=clean,
    visualize as scatter/.list={d},
    visualize as line/.list={d1},
    d={style={mark=x, mark size=1.5pt}},
    d1={style={visualizer color=black!50},
        label in legend={text={linear regression line} }},
    y axis={include value={0}, label={amount of iterations $\#i$}},
    x axis={include value={0}, label={time in seconds $t$}},
    %legend=below,
  ]
  data[headline={x, y}, read from file=data/time_sum_num_iter.csv, set=d]
  data[headline={x, y}, read from file=data/lin_reg_time_sum_num_iter.csv, set=d1]
  ;
\end{tikzpicture}
\caption{Plot showing the correlation of the execution time
of Algorithm~\ref{alg:perc} and the amounts of iterations
of Algorithm~\ref{alg:build_clusters}.}
\label{fig:p1:e}
\end{subfigure}
% }}}

\caption{Results of the first phase of the benchmark.}
\label{fig:p1}
\end{center}
\end{figure}
% }}}

The overall execution time $t$ in seconds (the sum
of the execution time of the three operations, see previous
chapter) gets mapped onto the corresponding density $\rho$
of $A$ (see Figure~\ref{fig:p1}).
The benchmark also looks at how the three different
measured operations influence $t$, to see how they scale
and to further underline where the bottlenecks lie.

The relationship between $\rho$ and the execution time
is clearly not linear---like expected---and behaves rather
extreme (see Figure~\ref{fig:p1:a}).
The program produces a discontinuity like jump at $\rho
\approx 0.58$.
While the overall average $t$ is approximately
$278.49$ seconds, the average $t$ over all measures where
$\rho \leq 0.58$ is just approximately $14.79$ seconds,
while the average $t$ over all measures where $\rho > 0.58$
is approximately $639.33$ seconds.
The average of the measurements where $\rho \leq 0.58$ is
a staggering 2 percent of the average of the measurements
where $\rho > 0.58$.
The density of the execution time is a third factor that
makes the measurement extreme (see Figure~\ref{fig:p1:b}).
Added to this jumping behavior and the scale, the
distribution of the execution time also favors extreme
timings.
The upper and lower quartiles of the distribution of $t$
contain 50 percent of all measurements.
This means, not only is there an extreme jump from very
low execution time to very high, there are also just a
few time measurements that are close to the overall average
execution time of approximately $278.49$ seconds.

Figures~\ref{fig:p1:c} and \ref{fig:p1:d} show the same
relation, $t$ replaced with the amount of iterations $\#i$
of the while-loop in Algorithm~\ref{alg:build_clusters}.
The plots look rather similar to each other.
Figure~\ref{fig:p1:e} shows the relation between $t$ and
$\#i$ and it is clearly linear.
The Pearson correlation coefficient is approximately
$0.999$, very close to a perfect linear relationship,
making it save to say, that $\#i$ is the main cause for
$t$.
The relation is not perfectly linear, because $t$ drops
slightly with higher $\rho$ ($\rho > 0.58$), while $\#i$
stays constant over the same interval (see
Figures~\ref{fig:p1:a}--\ref{fig:p1:d}).

% 2nd part fig {{{
\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.75] % {{{
\begin{axis}[
  tick align=outside,
  axis x line=bottom,
  axis y line=left,
  axis line shift=10pt,
  xlabel=$\rho$,
  ylabel=percentage of $t$,
  label shift=10pt,
  legend style={
    at={(2.5,0.5)},
    anchor=east,
    cells={anchor=west},
    draw=white
  },
]
  \addplot[name path=map, color=black!30] table[header=false, col sep=comma] {data/map_percentage.csv};
  \addlegendentry{build\_clusters($A'$)}
  \addplot[name path=sort, color=black!60] table[header=false, col sep=comma] {data/sort_percentage.csv};
  \addlegendentry{sort the clusters}
  \addplot[name path=color, color=black!90] table[header=false, col sep=comma] {data/color_percentage.csv};
  \addlegendentry{build color map}

  \path[name path=axis] (axis cs:0.009,0) -- (axis cs:0.99,0);

  \addplot[fill=black!30] fill between[of=map and sort];
  \addplot[fill=black!60] fill between[of=sort and color];
  \addplot[fill=black!90] fill between[of=color and axis];
\end{axis}
\end{tikzpicture}
% }}}
\vspace{0.5cm}
\caption{Percentage of $t$ spent for every measured
  operation in Algorithm~\ref{alg:perc}.}
\label{fig:p2}
\end{center}
\end{figure}
% }}}

The fact that Algorithm~\ref{alg:build_clusters} is the
main reason for $t$---and therefore the bottleneck---is
underlined by looking at the three different measured
operations (clustering, sorting and building the color map)
distinctly.
Especially for the extreme high timings (for $\rho >
0.58$), approximately 99 percent of $t$ was spent on the
clustering (see Figure~\ref{fig:p2}).

For the lower densities---which produce very low
$t$---the clustering takes approximately linearly less with
lower $\rho$.
At its lowest point, the clustering only takes
approximately 45 percent of $t$, while the rest of the time
is spent sorting the clusters.

The building of the color map is constant over $\rho$.
The average time spent on building the clusters is 0.05
seconds and does not deviate much with a standard deviation
of 0.04 seconds.
The sorting takes slightly more time than the building of
the color map, but it is also constant over $\rho$ and
takes only 0.71 seconds on average with a standard
deviation of 0.12 seconds.
Both operations can be deemed irrelevant to $t$ concerning
its scale.
Sorting only plays a role for very low $\rho$ which also
produce very low $t$.

% }}}

\section{Discussion} % {{{

On the one hand, this benchmark sufficiently answers the
question, where the performance bottleneck of \perc{} \v{}
lies.
Clearly Algorithm~\ref{alg:build_clusters} is the main
source of computation and therefore execution time.

On the other hand, this benchmark raises the question, why
the scaling over $\rho$ is not linear and favors such
extreme $t$, with a abrupt jump at $\rho \approx 0.58$.
This question is not answered by the benchmark and needs to
be further evaluated.

The most obvious optimization strategy for \perc{}---based
on this benchmark---would be to reduce the high execution
time for more dense $A$.

This could be done by the following possible optimizations:
\begin{itemize}
  \item Update $\mu$ to contain the diagonal neighbors as
        well. This would reduce the amount of iterations,
        respectively reducing the maximum distance of two
        cells in the matrix from $n^2$ to $n$.

  \item Parallelizing the inner loop of
        Algorithm~\ref{alg:build_clusters}.

  \item Not using an iterative approach. Instead of
        iterating over the matrix, one could use a priority
        queue, which prioritizes higher $\mathbb{N}$
        and points to all the empty cells of $A$.
        This empty cell with the highest value is taken and
        all neighbors are recursively updated.
        The updated cells are removed from the queue.
        If the update process stops, the now highest value
        from the queue is taken and the update
        process is done again.
        This is continued until the queue is empty.

        Since taking the highest value is not even a
        necessity (it was just used as the criteria for
        an early return from the while-loop of
        Algorithm~\ref{alg:build_clusters}), one could
        even replace the priority queue with a different,
        more easy to parallelize, data structure to further
        optimize \perc.

\end{itemize}
If these proposed optimizations can reduce the execution
time remains to be tested.

Another open question not answered is the divergence
between the amount of iterations of the while-loop of
Algorithm~\ref{alg:build_clusters} and the execution time
for $\rho > 0.58$
(see Figures~\ref{fig:p1:a},~\ref{fig:p1:c}).
While the amount of iterations stays constant on this
interval, the execution time drops with bigger $\rho$.
This is suggestive for another factor influencing the
execution time.
This benchmark has not tested the amounts of clusters or
their size and how they influence the execution time,
which could also be a factor.

% }}}

\section{Conclusion} % {{{

\perc{} \v{} scales rather extreme and unexpected over
different densities $\rho$:

\begin{enumerate}
  \item A discontinuity like jump for $\rho \approx 0.58$.

  \item The scale of the execution time.
        The average execution time for $\rho > 0.58$ is
        approximately $639.33$ seconds, while the average
        execution time for $\rho \leq 0.58$ lies at
        approximately 14.79 seconds---a staggering 2
        percent of the average execution time for
        $\rho > 0.58$.

  \item The distribution of the execution time favors
        the extremes (both minimum and maximum). 50 percent
        of all measurements lie in the outer quartiles.
\end{enumerate}

Even though this benchmark raises some unanswered
questions (most of all the scaling behavior), it shows the
bottleneck of the execution time in
Algorithm~\ref{alg:build_clusters}.
This allows for precise optimization of \perc{} in order to
improve its performance, especially for $\rho > 0.58$.
The previous chapter outlined different approaches, which
can be taken in order to optimize \perc{}.

On the other hand the unanswered questions must be
addressed in a follow up benchmark, possibly combined with
the results of the optimized version of \perc{}, which may
scale differently.

The next tasks for optimizing \perc{} will be to set up
a thorough benchmark suite for regression testing, before
optimizations will be tested (\perc{} \texttt{v0.1.1}).
Hopefully, \perc{} \texttt{v0.1.2} will already contain
an optimized version of Algorithm~\ref{alg:build_clusters},
making it perform and scale better.

% }}}

\bibliography{pscw.bib}

\end{document}
